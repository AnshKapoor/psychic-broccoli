#!/bin/bash -l
#
# ========= Phoenix / Slurm: backbone clustering (full mode) =========
# Uses virtual environment: "psychic"
# ====================================================================

#### ----- Slurm resources & naming -----
# Adjust resources if your dataset is large; this is a reasonable starting point.
#SBATCH --job-name=backbone_full
#SBATCH --partition=standard
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8          # clustering + UTM + resampling
#SBATCH --mem=32G                  # increase if you load many months at once
#SBATCH --time=24:00:00
#SBATCH --output=logs/%x_%j.out
#SBATCH --error=logs/%x_%j.err

# Optional notifications:
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=a.kapoor@tu-braunschweig.de

set -eo pipefail
umask 027

echo "[info] job=$SLURM_JOB_NAME id=$SLURM_JOB_ID node=$SLURM_JOB_NODELIST"
echo "[info] part=$SLURM_JOB_PARTITION cpus=$SLURM_CPUS_PER_TASK mem=$SLURM_MEM_PER_NODE"

#### ----- Activate your venv -----
source ~/venvs/psychic/bin/activate

module purge
module load lib/openssl/latest
module load python/3.9.7

# activate the venv that was created with that module
source ~/venvs/psychic/bin/activate

# optional: fail early if wrong interpreter
python - <<'PY'
import sys
assert sys.version_info >= (3,8), f"Need Python >=3.8, got {sys.version}"
PY

which python
python --version

# Limit threading to match Slurm allocation
export OMP_NUM_THREADS="${SLURM_CPUS_PER_TASK:-1}"
export MKL_NUM_THREADS="${SLURM_CPUS_PER_TASK:-1}"
export OPENBLAS_NUM_THREADS="${SLURM_CPUS_PER_TASK:-1}"
export NUMEXPR_NUM_THREADS="${SLURM_CPUS_PER_TASK:-1}"

#### ----- Run your script -----
cd /home/y0113724/psychic-broccoli

mkdir -p logs/python

# Full-mode config (testing.enabled: false)
srun /home/y0113724/venvs/psychic/bin/python cli.py \
  -c config/backbone_full.yaml

