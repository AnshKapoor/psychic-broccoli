#!/bin/bash -l
#
# ========= Phoenix / Slurm: visualize pipeline outputs =========
# Uses virtual environment: "psychic"
# ================================================================

#### ----- Slurm resources & naming -----
# Lightweight plotting job; adjust if your outputs are large.
# 2 CPUs, 8 GB RAM, 1 hour walltime should suffice for PNG generation.
# Name will appear in Slurm queue and log filenames.
#SBATCH --job-name=visualize_outputs
#SBATCH --partition=standard
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=2
#SBATCH --mem=8G
#SBATCH --time=01:00:00
#SBATCH --output=logs/%x_%j.out
#SBATCH --error=logs/%x_%j.err

# Optional notifications:
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=a.kapoor@tu-braunschweig.de

set -eo pipefail
umask 027

echo "[info] job=$SLURM_JOB_NAME id=$SLURM_JOB_ID node=$SLURM_JOB_NODELIST"
echo "[info] part=$SLURM_JOB_PARTITION cpus=$SLURM_CPUS_PER_TASK mem=$SLURM_MEM_PER_NODE"

#### ----- Activate your venv -----
# Make sure this matches the location of your existing environment.
source ~/venvs/psychic/bin/activate

module purge
module load lib/openssl/latest
module load python/3.9.7

# activate the venv that was created with that module
source ~/venvs/psychic/bin/activate

# optional: fail early if wrong interpreter
python - <<'PY'
import sys
assert sys.version_info >= (3,8), f"Need Python >=3.8, got {sys.version}"
PY

which python
python --version

# Limit threading to match Slurm allocation
export OMP_NUM_THREADS="${SLURM_CPUS_PER_TASK:-1}"
export MKL_NUM_THREADS="${SLURM_CPUS_PER_TASK:-1}"
export OPENBLAS_NUM_THREADS="${SLURM_CPUS_PER_TASK:-1}"
export NUMEXPR_NUM_THREADS="${SLURM_CPUS_PER_TASK:-1}"

#### ----- Run your script -----
cd /home/y0113724/psychic-broccoli

mkdir -p logs/python

# Adjust experiment prefix to pick the desired CSV set in output/run/csv
srun /home/y0113724/venvs/psychic/bin/python visualize_outputs.py \
  --experiment-prefix OPTICS_exp_3
