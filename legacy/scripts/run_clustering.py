"""Command-line helper for clustering matched ADS-B trajectories.

The goal of this template is to demonstrate the core wiring needed to take the
CSV generated by :func:`merge_adsb_noise.match_noise_to_adsb`, aggregate the
per-flight trajectories, engineer simple numeric features, and feed them into a
baseline clustering algorithm. Replace or extend the building blocks as your
analysis matures (e.g., alternative feature extraction, dimensionality
reduction, or more advanced clustering techniques).
"""

from __future__ import annotations

import argparse
import json
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, Iterable, Iterator, List, Sequence

import pandas as pd
import yaml
from sklearn.cluster import DBSCAN, Birch, KMeans, OPTICS
from sklearn.preprocessing import StandardScaler

from src.data.trajectory_preprocessing import preprocess_trajectory


@dataclass
class AlgorithmConfig:
    """Container for algorithm-specific parameters."""

    name: str = "kmeans"
    n_clusters: int = 5
    random_state: int | None = 42
    max_iter: int = 300
    eps: float = 0.5
    min_samples: int = 10
    metric: str = "euclidean"


@dataclass
class PreprocessConfig:
    """Rules that clean and harmonise the input CSV before clustering."""

    required_columns: Sequence[str] = (
        "timestamp",
        "latitude",
        "longitude",
        "altitude",
        "groundspeed",
        "track",
        "dist_to_airport_m",
    )
    numeric_impute_median: Sequence[str] = ("vertical_rate", "geoaltitude")
    numeric_impute_zero: Sequence[str] = ()
    categorical_fill_value: Dict[str, str] = field(
        default_factory=lambda: {
            "origin": "UNKNOWN",
            "destination": "UNKNOWN",
            "aircraft_type_adsb": "unknown",
            "aircraft_type_noise": "unknown",
            "aircraft_type_match": "unknown",
        }
    )
    boolean_fill_value: Dict[str, bool] = field(default_factory=lambda: {"onground": False})
    drop_columns: Sequence[str] = ("squawk", "last_position", "firstseen", "lastseen", "hour", "day")


@dataclass
class FeatureConfig:
    """Options that control the feature engineering stage."""

    numeric_columns: Sequence[str] = (
        "latitude",
        "longitude",
        "altitude",
        "groundspeed",
        "vertical_rate",
        "track",
        "dist_to_airport_m",
    )
    summary_statistics: Sequence[str] = ("mean", "std", "min", "max")
    target_length: int = 20


@dataclass
class RunConfig:
    """Full configuration schema loaded from YAML."""

    input_csv: Path
    output_dir: Path = Path("reports")
    group_keys: Sequence[str] = ("MP", "t_ref", "icao24")
    preprocessing: PreprocessConfig = field(default_factory=PreprocessConfig)
    algorithm: AlgorithmConfig = field(default_factory=AlgorithmConfig)
    features: FeatureConfig = field(default_factory=FeatureConfig)


def load_config(path: Path) -> RunConfig:
    """Read the YAML config file and map it into dataclasses."""

    with path.open("r", encoding="utf-8") as fh:
        raw_cfg = yaml.safe_load(fh) or {}

    preprocess_cfg = PreprocessConfig(**raw_cfg.get("preprocessing", {}))
    algorithm_cfg = AlgorithmConfig(**raw_cfg.get("algorithm", {}))
    feature_cfg = FeatureConfig(**raw_cfg.get("features", {}))
    cfg = RunConfig(
        input_csv=Path(raw_cfg["input_csv"]),
        output_dir=Path(raw_cfg.get("output_dir", "reports")),
        group_keys=tuple(raw_cfg.get("group_keys", ("MP", "t_ref", "icao24"))),
        preprocessing=preprocess_cfg,
        algorithm=algorithm_cfg,
        features=feature_cfg,
    )
    return cfg


def _iter_preprocessed_groups(
    df: pd.DataFrame,
    group_keys: Sequence[str],
    target_length: int,
) -> Iterator[tuple[pd.Series, pd.DataFrame]]:
    """Yield (group_metadata, processed_trajectory) pairs."""

    missing = [key for key in group_keys if key not in df.columns]
    if missing:
        raise KeyError(f"Input CSV missing grouping columns: {missing}")

    for group_values, group_df in df.groupby(list(group_keys)):
        processed = preprocess_trajectory(group_df, target_length=target_length)
        if processed.empty:
            continue
        meta = pd.Series(group_values, index=group_keys)
        yield meta, processed


def _summarise_numeric_series(values: pd.Series, statistics: Sequence[str]) -> Dict[str, float]:
    """Compute simple summary statistics for a numeric series."""

    summary: Dict[str, float] = {}
    for stat in statistics:
        if stat == "mean":
            summary["mean"] = float(values.mean())
        elif stat == "std":
            summary["std"] = float(values.std(ddof=0))
        elif stat == "min":
            summary["min"] = float(values.min())
        elif stat == "max":
            summary["max"] = float(values.max())
        elif stat == "median":
            summary["median"] = float(values.median())
        else:
            raise ValueError(f"Unsupported statistic '{stat}'")
    return summary


def build_feature_table(
    processed_groups: Iterable[tuple[pd.Series, pd.DataFrame]],
    numeric_columns: Sequence[str],
    statistics: Sequence[str],
) -> pd.DataFrame:
    """Convert a sequence of processed trajectory slices into feature vectors."""

    feature_rows: List[Dict[str, object]] = []
    for meta, traj in processed_groups:
        row: Dict[str, object] = meta.to_dict()
        for column in numeric_columns:
            if column not in traj.columns:
                continue
            stats = _summarise_numeric_series(traj[column].astype(float), statistics)
            for stat_name, value in stats.items():
                row[f"{column}_{stat_name}"] = value
        feature_rows.append(row)

    if not feature_rows:
        raise ValueError("No trajectory groups produced usable features.")
    return pd.DataFrame(feature_rows)


def run_kmeans(feature_df: pd.DataFrame, cfg: AlgorithmConfig) -> pd.Series:
    """Execute a basic KMeans clustering pass."""

    numeric_cols = feature_df.select_dtypes(include="number").columns
    scaler = StandardScaler()
    scaled = scaler.fit_transform(feature_df[numeric_cols])

    model = KMeans(
        n_clusters=cfg.n_clusters,
        random_state=cfg.random_state,
        max_iter=cfg.max_iter,
        n_init="auto",
    )
    labels = model.fit_predict(scaled)
    feature_df = feature_df.copy()
    feature_df["cluster"] = labels
    return feature_df["cluster"]


def run_dbscan(feature_df: pd.DataFrame, cfg: AlgorithmConfig) -> pd.Series:
    """Density-based clustering that handles non-globular clusters."""

    numeric_cols = feature_df.select_dtypes(include="number").columns
    scaled = StandardScaler().fit_transform(feature_df[numeric_cols])

    model = DBSCAN(eps=cfg.eps, min_samples=cfg.min_samples, metric=cfg.metric)
    labels = model.fit_predict(scaled)
    return pd.Series(labels, index=feature_df.index, name="cluster")


def run_optics(feature_df: pd.DataFrame, cfg: AlgorithmConfig) -> pd.Series:
    """OPTICS clustering for variable-density trajectory groups."""

    numeric_cols = feature_df.select_dtypes(include="number").columns
    scaled = StandardScaler().fit_transform(feature_df[numeric_cols])

    model = OPTICS(min_samples=cfg.min_samples, max_eps=cfg.eps or float("inf"), metric=cfg.metric)
    labels = model.fit_predict(scaled)
    return pd.Series(labels, index=feature_df.index, name="cluster")


def run_birch(feature_df: pd.DataFrame, cfg: AlgorithmConfig) -> pd.Series:
    """Birch clustering for large, noise-tolerant datasets."""

    numeric_cols = feature_df.select_dtypes(include="number").columns
    scaled = StandardScaler().fit_transform(feature_df[numeric_cols])

    model = Birch(n_clusters=cfg.n_clusters if cfg.n_clusters > 0 else None)
    labels = model.fit_predict(scaled)
    return pd.Series(labels, index=feature_df.index, name="cluster")


def dispatch_clustering(feature_df: pd.DataFrame, cfg: AlgorithmConfig) -> pd.Series:
    """Route to the configured clustering algorithm."""

    match cfg.name.lower():
        case "kmeans":
            return run_kmeans(feature_df, cfg)
        case "dbscan":
            return run_dbscan(feature_df, cfg)
        case "optics":
            return run_optics(feature_df, cfg)
        case "birch":
            return run_birch(feature_df, cfg)
        case _:
            raise ValueError(f"Unsupported clustering algorithm: {cfg.name}")


def attach_clusters(feature_df: pd.DataFrame, labels: pd.Series) -> pd.DataFrame:
    """Append the cluster labels to the feature table."""

    result = feature_df.copy()
    result["cluster"] = labels.values
    return result


def save_outputs(
    feature_df: pd.DataFrame,
    cfg: RunConfig,
) -> None:
    """Persist features, labels, and a machine-readable metadata file."""

    cfg.output_dir.mkdir(parents=True, exist_ok=True)
    features_path = cfg.output_dir / "cluster_features.csv"
    metadata_path = cfg.output_dir / "clustering_metadata.json"

    feature_df.to_csv(features_path, index=False)
    metadata = {
        "algorithm": cfg.algorithm.__dict__,
        "group_keys": list(cfg.group_keys),
        "preprocessing": {
            "required_columns": list(cfg.preprocessing.required_columns),
            "numeric_impute_median": list(cfg.preprocessing.numeric_impute_median),
            "numeric_impute_zero": list(cfg.preprocessing.numeric_impute_zero),
            "categorical_fill_value": cfg.preprocessing.categorical_fill_value,
            "boolean_fill_value": cfg.preprocessing.boolean_fill_value,
            "drop_columns": list(cfg.preprocessing.drop_columns),
        },
        "numeric_columns": list(cfg.features.numeric_columns),
        "statistics": list(cfg.features.summary_statistics),
    }
    metadata_path.write_text(json.dumps(metadata, indent=2), encoding="utf-8")


def preprocess_input_table(df: pd.DataFrame, cfg: RunConfig) -> pd.DataFrame:
    """Clean raw matched trajectories based on column-level policies."""

    proc_cfg = cfg.preprocessing
    df_proc = df.copy()

    required = set(proc_cfg.required_columns).union(cfg.group_keys)
    missing_columns = [col for col in required if col not in df_proc.columns]
    if missing_columns:
        raise KeyError(f"Input CSV missing required columns: {missing_columns}")

    # Coerce numeric columns to floats and drop rows missing required fields.
    numeric_candidates = set(cfg.features.numeric_columns).union(
        proc_cfg.numeric_impute_median, proc_cfg.numeric_impute_zero
    )
    for col in numeric_candidates:
        if col in df_proc.columns:
            df_proc[col] = pd.to_numeric(df_proc[col], errors="coerce")
    if "timestamp" in df_proc.columns:
        df_proc["timestamp"] = pd.to_datetime(df_proc["timestamp"], errors="coerce")

    df_proc = df_proc.dropna(subset=required)

    # Fill optional numeric/categorical columns so later steps see consistent values.
    for col in proc_cfg.numeric_impute_median:
        if col in df_proc.columns:
            median = df_proc[col].median()
            df_proc[col] = df_proc[col].fillna(median)
    for col in proc_cfg.numeric_impute_zero:
        if col in df_proc.columns:
            df_proc[col] = df_proc[col].fillna(0)
    for col, value in proc_cfg.categorical_fill_value.items():
        if col in df_proc.columns:
            df_proc[col] = df_proc[col].fillna(value)
    for col, value in proc_cfg.boolean_fill_value.items():
        if col in df_proc.columns:
            df_proc[col] = df_proc[col].fillna(value).astype(bool)

    drop_cols = [col for col in proc_cfg.drop_columns if col in df_proc.columns]
    if drop_cols:
        df_proc = df_proc.drop(columns=drop_cols)

    return df_proc


def parse_args() -> argparse.Namespace:
    """Return CLI arguments."""

    parser = argparse.ArgumentParser(description="Cluster matched ADS-B trajectories.")
    parser.add_argument(
        "--config",
        type=Path,
        default=Path("config/params.yaml"),
        help="Path to the YAML config file.",
    )
    return parser.parse_args()


def main() -> None:
    """CLI entry point."""

    args = parse_args()
    cfg = load_config(args.config)

    df = pd.read_csv(cfg.input_csv)
    df = preprocess_input_table(df, cfg)
    groups = list(
        _iter_preprocessed_groups(
            df,
            group_keys=cfg.group_keys,
            target_length=cfg.features.target_length,
        )
    )
    feature_df = build_feature_table(
        processed_groups=groups,
        numeric_columns=cfg.features.numeric_columns,
        statistics=cfg.features.summary_statistics,
    )
    labels = dispatch_clustering(feature_df, cfg.algorithm)
    clustered = attach_clusters(feature_df, labels)
    save_outputs(clustered, cfg)


if __name__ == "__main__":
    main()
